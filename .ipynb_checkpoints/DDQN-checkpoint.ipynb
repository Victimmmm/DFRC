{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9eP0QeyYpVHL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.optimizers import Adam\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvFReeTTpfC1"
   },
   "source": [
    "#Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wLpTVhDbpeV_"
   },
   "outputs": [],
   "source": [
    "class DoubleDeepQNetwork():\n",
    "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
    "        self.nS = states\n",
    "        self.nA = actions\n",
    "        self.memory = deque([], maxlen=2000)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        #Explore/Exploit\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.model = self.build_model()\n",
    "        self.model_target = self.build_model() #Second (target) neural network\n",
    "        self.update_target_from_model() #Update weights\n",
    "        self.loss = []\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = keras.Sequential() #linear stack of layers https://keras.io/models/sequential/\n",
    "        model.add(keras.layers.Dense(32, input_dim=self.nS, activation='relu')) #[Input] -> Layer 1\n",
    "        #   Dense: Densely connected layer https://keras.io/layers/core/\n",
    "        #   24: Number of neurons\n",
    "        #   input_dim: Number of input variables\n",
    "        #   activation: Rectified Linear Unit (relu) ranges >= 0\n",
    "        model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "        model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "        model.add(keras.layers.Dense(self.nA, activation='linear')) #Layer 3 -> [output]\n",
    "        #   Size has to match the output (different actions)\n",
    "        #   Linear activation on the last layer\n",
    "        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n",
    "                      optimizer=keras.optimizers.Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n",
    "        return model\n",
    "\n",
    "    def update_target_from_model(self):\n",
    "        #Update the target model from the base model\n",
    "        self.model_target.set_weights( self.model.get_weights() )\n",
    "\n",
    "    def action(self, env, state):\n",
    "        A = np.random.random()\n",
    "        if A <= self.epsilon:\n",
    "          action = env.sam_action() #Explore\n",
    "          Flag = 0\n",
    "          return action, Flag\n",
    "        else:        \n",
    "          action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
    "          act_idx = np.argmax(action_vals[0])\n",
    "          action = env.index2action(act_idx)\n",
    "          Flag = 1\n",
    "          return action, Flag\n",
    "        \n",
    "\n",
    "    def test_action(self, env,state): #Exploit\n",
    "        action_vals = self.model.predict(state)\n",
    "        act_idx = np.argmax(action_vals[0])\n",
    "        action = env.index2action(act_idx)\n",
    "        return action \n",
    "\n",
    "    def store(self, state, action, reward, nstate, done):\n",
    "        #Store the experience in memory\n",
    "        self.memory.append( (state, action, reward, nstate, done) )\n",
    "\n",
    "    def experience_replay(self, env, batch_size):\n",
    "        #Execute the experience replay\n",
    "        minibatch = random.sample( self.memory, batch_size) #Randomly sample from memory\n",
    "\n",
    "        #Convert to numpy for speed by vectorization\n",
    "        x = []\n",
    "        y = []\n",
    "        np_array = np.array(minibatch)\n",
    "        st = np.zeros((0,self.nS)) #States\n",
    "        nst = np.zeros( (0,self.nS) )#Next States\n",
    "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
    "            st = np.append( st, np_array[i,0], axis=0)\n",
    "            nst = np.append( nst, np_array[i,3], axis=0) # Because the store includes state, action, reward, nstate, and done\n",
    "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
    "        #print(f\"st_predict:{st_predict}\")\n",
    "        nst_predict = self.model.predict(nst)\n",
    "        nst_predict_target = self.model_target.predict(nst) #Predict from the TARGET\n",
    "        index = 0\n",
    "        for state, action, reward, nstate, done in minibatch:\n",
    "            x.append(state)\n",
    "            #Predict from state\n",
    "            nst_action_predict_target = nst_predict_target[index]\n",
    "            nst_action_predict_model = nst_predict[index]\n",
    "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
    "                target = reward\n",
    "            else:   #Non terminal\n",
    "                target = reward + self.gamma * nst_action_predict_target[np.argmax(nst_action_predict_model)] #Using Q to get T is Double DQN\n",
    "\n",
    "            #print(f\"target:{target}\") \n",
    "            target_f = st_predict[index]\n",
    "            #print(f\"target_f:{target_f}\")  \n",
    "            #print(f\"size target_f:{target_f.shape}\") \n",
    "            idx = env.action2index(action)\n",
    "            #print(f\"idx:{int(idx)}\") \n",
    "            #print(f\"target_f[2]:{target_f[2]}\") \n",
    "            target_f[int(idx)] = target\n",
    "            y.append(target_f)\n",
    "            index += 1\n",
    "        #Reshape for Keras Fit\n",
    "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
    "        y_reshape = np.array(y)\n",
    "        epoch_count = 1\n",
    "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
    "        #Graph Losses\n",
    "        for i in range(epoch_count):\n",
    "            self.loss.append( hist.history['loss'][i] )\n",
    "        #Decay Epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return self.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvCxdRBFpoij"
   },
   "source": [
    "#Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1606,
     "status": "ok",
     "timestamp": 1658121048302,
     "user": {
      "displayName": "Linh Nguyen Thi Hoai",
      "userId": "09006753140999843619"
     },
     "user_tz": -420
    },
    "id": "kMgY3wr8pqsv",
    "outputId": "089d942a-ed8e-4afd-d654-85e0e33cc8f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.0\n",
      "[2, 0.0, 0.0, 1.0, 0.0, 0.0, 1] [2, 0, 0, 1, 0, 0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "class Helper:\n",
    "    def __init__(self, rc, max_f, max_c):\n",
    "        \"\"\"\n",
    "        Initial Method for Continuous Environment\n",
    "        params - rc: float - Radius Cycle\n",
    "        params - max_f: float - Maximum CPUs\n",
    "        params - max_c: float - Maximum Costs\n",
    "        \"\"\"\n",
    "        if rc < 0 or max_f < 0 or max_c < 0:\n",
    "            raise Exception(\n",
    "                    \"Initial Values for Helper must be Positive!\"\n",
    "                )\n",
    "        self.rc = rc\n",
    "        self.max_f = max_f\n",
    "        self.max_c = max_c\n",
    "\n",
    "        self.f = None\n",
    "        self.c = None\n",
    "\n",
    "    def become_stranger(self):\n",
    "        \"\"\"This node become to a Stranger Node\"\"\"\n",
    "        f_frac = np.random.uniform(low=1e-6, high=1e-5)\n",
    "        c_frac = np.random.uniform(low=0.8, high=1.0)\n",
    "        self.f = self.f * f_frac\n",
    "\n",
    "        if np.random.rand() < 0.9:\n",
    "            self.c = self.c  * f_frac * np.random.uniform(low=10.0, high=20.0)\n",
    "        else:\n",
    "            self.c = self.c * f_frac\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Create a New Instance\n",
    "        \"\"\"\n",
    "        self.f = np.random.normal(loc=self.max_f * 0.5, scale=self.max_f * 1e-2)\n",
    "        self.c = np.random.normal(loc=self.max_c * 0.5, scale=self.max_c * 1e-2)\n",
    "\n",
    "        if np.random.rand() < 0.1:\n",
    "            self.become_stranger()\n",
    "\n",
    "        if self.f < 0:\n",
    "            self.f = self.max_f * 0.5\n",
    "        if self.c < 0:\n",
    "            self.c = self.max_c * 0.5\n",
    "\n",
    "    def transit(self):\n",
    "        \"\"\"\n",
    "        Move to a Next State\n",
    "        \"\"\"\n",
    "        self.f = np.random.normal(loc=self.f, scale=self.max_f * 1e-2)\n",
    "        self.c = np.random.normal(loc=self.c, scale=self.max_c * 1e-2)\n",
    "\n",
    "        if self.f < 0:\n",
    "            self.f = self.max_f * 0.5\n",
    "\n",
    "        if self.c < 0:\n",
    "            self.c = self.max_c * 0.5\n",
    "\n",
    "        if np.random.rand() < 0.1:\n",
    "            self.become_stranger()\n",
    "\n",
    "    def cal_com_latency(self, num_bytes):\n",
    "        \"\"\"\n",
    "        Calculate The Latency for Computing \"num_bytes\" data \n",
    "        params: num_bytes - Integer - Computation Demand\n",
    "        \"\"\"\n",
    "        num_bytes = float(num_bytes)\n",
    "        latency = np.random.normal(loc=num_bytes / self.f, scale=num_bytes / self.f * 1e-2)\n",
    "        return latency\n",
    "\n",
    "    def cal_offload_latency(self, num_bytes, B_c, m, d_n):\n",
    "        \"\"\"\n",
    "        Calculate The Latency for Offloading \"num_bytes\" data\n",
    "        params: num_bytes - Integer - Offloading Demand\n",
    "        \"\"\"\n",
    "        num_bytes = float(num_bytes)\n",
    "        # Transformation Parameters\n",
    "        CO = 3*1e8\n",
    "        FB = 1e8\n",
    "        PT = 0.25\n",
    "        sigma = 0.5\n",
    "\n",
    "        pr = PT * (CO** 2) / (((np.pi ** 4) * FB * d_n) ** 2)\n",
    "        rn = (B_c/m) * np.log(1.0 + pr /sigma) \n",
    "        latency = num_bytes / rn\n",
    "        return latency\n",
    "\n",
    "    def cal_incentive_cost(self, num_bytes):\n",
    "        \"\"\"\n",
    "        Calculate the Incentive Cost for Processing \"num_bytes\" data\n",
    "        params: - num_bytes : Integer\n",
    "        \"\"\"\n",
    "        cost = self.c * self.cal_com_latency(num_bytes)\n",
    "        return cost\n",
    "\n",
    "    def show_cur_state(self):\n",
    "        print(\"d: {:.3f}, f: {:.3f}, c: {:.10f}\".format(self.d, self.f, self.c))\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the Current State of This Helper\n",
    "        \"\"\"\n",
    "        state = [self.f/self.max_f, self.c/self.max_c]\n",
    "\n",
    "        return state\n",
    "\n",
    "class TaskOffloadEnv:\n",
    "    def __init__(self, n_helpers, rc, max_f, max_c, max_l, alpha1, alpha2, alpha3, B, v_min, v_max,  seed=1):\n",
    "        \"\"\"\n",
    "        Initial Method for Task Offload Environments\n",
    "        \"\"\"\n",
    "        self.n_helpers = n_helpers\n",
    "\n",
    "        self.rc = rc\n",
    "        self.max_f = max_f\n",
    "        self.max_c = max_c\n",
    "        self.max_l = max_l\n",
    "\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.alpha3 = alpha3\n",
    "        self.B = B\n",
    "        self.helpers = {}\n",
    "        self.step_counter = 0\n",
    "\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "\n",
    "        self.curr_n_helpers = self.n_helpers\n",
    "\n",
    "        # environment's configuration\n",
    "        self.act_dims = [self.n_helpers] + [2 for _ in range(self.n_helpers)] + [3] \n",
    "        self.num_actions = self.n_helpers * (2 ** self.n_helpers) * 3\n",
    "        self.env_dims = 2 + self.n_helpers * 6\n",
    "\n",
    "        # Client initialization\n",
    "        self.x_0 = 0\n",
    "        self.y_0 = 0\n",
    "        self.v_x = 0\n",
    "        self.v_y = 0\n",
    "\n",
    "        self.userList_x = []\n",
    "        self.userList_y = []\n",
    "        self.userList_location = []\n",
    "        \n",
    "        self.userList_velocity_x = []\n",
    "        self.userList_velocity_y = []\n",
    "        self.userList_velocity = []\n",
    "\n",
    "        self.d_n = 0\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def _clientInit(self, ):\n",
    "      for user_no in range(self.n_helpers):\n",
    "        user_Radius = random.uniform(self.rc/2, self.rc)\n",
    "        line = [1,2,3]\n",
    "        line = random.choice(line)\n",
    "        if line == 1:\n",
    "          user_Angle  = math.asin(self.rc/2/user_Radius)\n",
    "        elif line == 2:\n",
    "          user_Angle  = math.asin(0/user_Radius)\n",
    "        else:\n",
    "          user_Angle  = math.asin(-self.rc/2/user_Radius)\n",
    "\n",
    "        user_x1 = self.x_0 + user_Radius * math.cos(user_Angle)\n",
    "        user_x2 = self.x_0 - user_Radius * math.cos(user_Angle)\n",
    "        user_x = random.choice([user_x1, user_x2])\n",
    "        user_y = self.y_0 + user_Radius * math.sin(user_Angle)\n",
    "\n",
    "        # velocity\n",
    "        user_vy= 0\n",
    "        user_vx = random.uniform(self.v_min , self.v_max)\n",
    "        self.userList_x.append(user_x)\n",
    "        self.userList_y.append(user_y)\n",
    "        self.userList_location.append([user_x, user_y])\n",
    "        self.userList_velocity.append([user_vx, user_vy])\n",
    "\n",
    "      # # Client Plot\n",
    "      # fig, ax = plt.subplots()                                                \n",
    "      # circle1 = plt.Circle((self.x_0, self.y_0), self.rc, color='b', fill=False) \n",
    "      # circle2 = plt.Circle((self.x_0, self.y_0), 1, color='r', fill=True)       \n",
    "      # ax.add_patch(circle1)\n",
    "      # ax.add_patch(circle2)\n",
    "      # print(f\"List of user location is: {self.userList_location}\")\n",
    "      # print(f\"List of user velocity is: {self.userList_velocity}\")\n",
    "      # plt.scatter(self.userList_x, self.userList_y)\n",
    "      # plt.show()\n",
    "    \n",
    "    def distance(self):\n",
    "      userList_distance = []            \n",
    "      # init client location:               \n",
    "      client_location = (self.x_0, self.y_0)\n",
    "      for user_no in range(self.n_helpers):\n",
    "        #print(f\"user_no {user_no}\")\n",
    "        user_distance = np.linalg.norm(np.array(self.userList_location[user_no]) - np.array(client_location))\n",
    "        userList_distance.append(user_distance)\n",
    "      #print(f\"List of user distance is: {userList_distance}\")\n",
    "      return userList_distance\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get Environment State\n",
    "        \"\"\"\n",
    "        \n",
    "        client_state = [self.l*1.0/self.max_l]\n",
    "        helper_state = []\n",
    "        userList_distance = self.distance()\n",
    "        num_helpers = 0\n",
    "        for key in sorted(list(self.helpers.keys())):                    \n",
    "          if userList_distance[key] > self.rc:\n",
    "            state_f_c = [0,0]\n",
    "          else:\n",
    "            helper = self.helpers[key]\n",
    "            state_f_c = helper.get_state()\n",
    "            num_helpers = num_helpers + 1\n",
    "          location = self.userList_location[key]\n",
    "          x = location[0]/self.rc\n",
    "          y = location[1]/self.rc\n",
    "          position = [x,y]\n",
    "          velocity = self.userList_velocity[key]\n",
    "          v_x = velocity[0]/self.v_max\n",
    "          v_y = velocity[1]/self.v_max\n",
    "          v= [v_x, v_y]\n",
    "          state = position + v + state_f_c\n",
    "          helper_state += state\n",
    "          \n",
    "        env_state = [num_helpers] + client_state + helper_state\n",
    "        #print(f\"env_state : {env_state}\")\n",
    "        return env_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Create a New Instance\n",
    "        \"\"\"\n",
    "        self._clientInit()\n",
    "        self.l = np.random.normal(loc=self.max_l * 0.5, scale=self.max_l * 1e-4)\n",
    "        if self.l < 0:\n",
    "            self.l = self.max_l * 0.5\n",
    "        self.step_counter = 0\n",
    "        for idx in range(self.n_helpers):\n",
    "            self.helpers[idx] = Helper(self.rc, self.max_f, self.max_c)\n",
    "            self.helpers[idx].reset()            \n",
    "        self.client_f = self.max_f * np.random.normal(loc=0.1, scale= 1e-4)\n",
    "\n",
    "        state = self.get_state()\n",
    "        #print(f\"List of user state is: {state}\")\n",
    "        return state\n",
    "\n",
    "    def Markov_chain(self,num_n_vehicles):\n",
    "\n",
    "      n_vehicle_state = {0 : \"N1\",1 : \"N2\",2 : \"N3\", 3 : \"N4\", 4 : \"N5\"}\n",
    "      State_1 = [0.4, 0.5, 0.05, 0.05, 0]\n",
    "      State_2 = [0.3, 0.2, 0.3, 0.1, 0.1]\n",
    "      State_3 = [0, 0.3, 0.3, 0.4, 0]\n",
    "      State_4 = [0.05, 0.05, 0.3, 0.3, 0.3]\n",
    "      State_5 = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "      transitionMatrix = np.array([State_1, State_2, State_3, State_4, State_5])\n",
    "      activityList = []\n",
    "      start_n_vehicles = num_n_vehicles - 1 \n",
    "      next_n_vehicles = np.random.choice([0,1,2,3,4], p = transitionMatrix[start_n_vehicles])\n",
    "      return next_n_vehicles + 1\n",
    "\n",
    "    def transmit(self, curr_n_vehicles):\n",
    "      # x = x + v_x*t, y = y + v_y*t\n",
    "      y_vecto = []\n",
    "      over_n_vehicles = 0\n",
    "      index = []\n",
    "      for user_no in range(self.n_helpers):\n",
    "        self.userList_location[user_no][0] = self.userList_location[user_no][0] + self.userList_velocity[user_no][0]\n",
    "        self.userList_location[user_no][1] = self.userList_location[user_no][1] + self.userList_velocity[user_no][1]\n",
    "\n",
    "      userList_distance = self.distance()\n",
    "      for user_no in range(self.n_helpers):\n",
    "        if userList_distance[user_no] > self.rc:\n",
    "          over_n_vehicles += 1\n",
    "          index.append(user_no)\n",
    "      remain_n_vehicles = curr_n_vehicles - over_n_vehicles\n",
    "\n",
    "      next_n_vehicles = self.Markov_chain(remain_n_vehicles)\n",
    "\n",
    "      if next_n_vehicles > remain_n_vehicles:\n",
    "        add_num = next_n_vehicles - remain_n_vehicles\n",
    "        #print(f\"add_num: {add_num}\")\n",
    "        for i in range(add_num):\n",
    "            idx = index[i]\n",
    "            self.userList_location[idx], self.userList_velocity[idx] = self.out_of_range(add_num) \n",
    "        curr_n_vehicles = next_n_vehicles\n",
    "\n",
    "      else:\n",
    "        curr_n_vehicles = remain_n_vehicles     \n",
    "      #print(f\"curr_n_vehicles is: {curr_n_vehicles}\")\n",
    "      for key in self.helpers.keys():\n",
    "        self.helpers[key].transit()\n",
    "\n",
    "      # fig, ax = plt.subplots()                                                \n",
    "      # circle1 = plt.Circle((self.x_0, self.y_0), self.rc, color='b', fill=False) \n",
    "      # circle2 = plt.Circle((self.x_0, self.y_0), 1, color='r', fill=True)       \n",
    "      # ax.add_patch(circle1)\n",
    "      # ax.add_patch(circle2)\n",
    "      # userList_x,userList_y = [],[]\n",
    "      # for user_no in range(self.n_helpers):\n",
    "      #   x = self.userList_location[user_no][0]\n",
    "      #   y= self.userList_location[user_no][1]\n",
    "      #   userList_x.append(x)\n",
    "      #   userList_y.append(y)\n",
    "      # plt.scatter(userList_x, userList_y)\n",
    "      # plt.show()\n",
    "    \n",
    "    def out_of_range(self, num):\n",
    "      # When vehicles go through the circle\n",
    "        user_Radius = random.uniform(self.rc/2, self.rc)\n",
    "        line = [1,2,3]\n",
    "        line = random.choice(line)\n",
    "        if line == 1:\n",
    "          user_Angle  = math.asin(self.rc/2/user_Radius)\n",
    "        elif line == 2:\n",
    "          user_Angle  = math.asin(0/user_Radius)\n",
    "        else:\n",
    "          user_Angle  = math.asin(-self.rc/2/user_Radius)\n",
    "\n",
    "        user_x = self.x_0 - user_Radius * math.cos(user_Angle)\n",
    "        user_y = self.y_0 + user_Radius * math.sin(user_Angle)\n",
    "\n",
    "        # velocity\n",
    "        user_vy= 0\n",
    "        user_vx = random.uniform(self.v_min , self.v_max)\n",
    "        location = [user_x, user_y]\n",
    "        velocity = [user_vx, user_vy]\n",
    "        return location, velocity\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action\n",
    "        action's format [k, a1, ..., aN, B_c]\n",
    "\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        c = 3*10**8\n",
    "        Lambda = 0.05\n",
    "        k = action[0]\n",
    "        if action[-1] == 1:\n",
    "          B_c = 0.3\n",
    "        elif action[-1] == 2:\n",
    "          B_c = 0.5\n",
    "        else:\n",
    "          B_c = 0.7\n",
    "        a_vec = action[-self.n_helpers-1:-1]\n",
    "        m = sum(a_vec)\n",
    "\n",
    "        standard_time = self.l / self.client_f\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        com_fee, total_latency = [], []\n",
    "        connect = []\n",
    "        num_bytes = self.l / k\n",
    "        userList_distance = self.distance()\n",
    "        for idx in sorted(list(self.helpers.keys())):\n",
    "            if a_vec[idx] == 0:\n",
    "                total_latency.append(np.Inf)\n",
    "                com_fee.append(0.0)\n",
    "            else:\n",
    "                helper = self.helpers[idx]\n",
    "                d_n = userList_distance[idx]\n",
    "                offload_latency = helper.cal_offload_latency(num_bytes, B_c * self.B, m, d_n)\n",
    "                com_latency = helper.cal_com_latency(num_bytes)\n",
    "                fee = helper.cal_incentive_cost(num_bytes)\n",
    "                com_fee.append(fee)\n",
    "                total_latency.append(offload_latency + com_latency)\n",
    "                duration = self.constrain(self.rc, self.userList_velocity[idx][0], 0 , self.userList_location[idx][0], 0)\n",
    "                connect.append(duration)\n",
    "\n",
    "        total_latency = sorted(total_latency)\n",
    "        required_latency = max(total_latency[:k])\n",
    "        required_fee = np.sum(com_fee)\n",
    "        required_anten =  c/(2*(1-B_c)*self.B)\n",
    "\n",
    "        required_connect = []\n",
    "        for i in range(len(connect)):\n",
    "          if connect[i] >= required_latency:\n",
    "            rw = 0\n",
    "          else:\n",
    "            rw =  Lambda\n",
    "          required_connect.append(rw)\n",
    "        connect_reward = np.sum(required_connect)\n",
    "        # Calculate in Case the action meets the conditions\n",
    "        if k <= m:\n",
    "            if required_latency > standard_time:\n",
    "                com_reward = -standard_time\n",
    "            else:\n",
    "                com_reward = standard_time - required_latency\n",
    "            com_reward = com_reward * self.alpha1\n",
    "            cost_reward = required_fee * self.alpha2\n",
    "            anten_reward =  required_anten*self.alpha3\n",
    "            total_reward = com_reward - cost_reward - anten_reward + connect_reward\n",
    "        else:\n",
    "            \"\"\"\n",
    "            an action doesn't meet the conditions\n",
    "            \"\"\"\n",
    "            com_reward = -1.0 * self.alpha1 * standard_time\n",
    "            cost_reward = 1.0 * self.alpha2 * required_fee\n",
    "            if m == 0:\n",
    "                cost_reward = (self.l / self.client_f) * self.max_c\n",
    "            anten_reward =  required_anten*self.alpha3\n",
    "            total_reward = com_reward - cost_reward - anten_reward + connect_reward\n",
    "        reward = [total_reward, com_reward, cost_reward, anten_reward, connect_reward]\n",
    "\n",
    "        \"\"\"\n",
    "        Move to the next State\n",
    "        \"\"\"\n",
    "        self.l = np.random.normal(loc=self.l, scale=self.max_l * 1e-4)\n",
    "        if self.l < 0:\n",
    "            self.l = self.max_l * 0.5\n",
    "\n",
    "        self. transmit(self.curr_n_helpers)\n",
    "        next_state = self.get_state()\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def sam_action(self):\n",
    "        \"\"\"\n",
    "        select one action randomly\n",
    "        action's format [k, a1, ..., aN]\n",
    "        \"\"\"\n",
    "        k = random.randint(1, self.n_helpers)\n",
    "        n = random.randint(1, self.n_helpers)\n",
    "        mylist = [0.3, 0.5, 0.7]\n",
    "        BC = []\n",
    "        for i in range(len(mylist)):\n",
    "          if mylist[i]==0.3:\n",
    "            bc = 1\n",
    "          elif mylist[i]==0.5:\n",
    "            bc = 2\n",
    "          else:\n",
    "            bc = 3\n",
    "          BC.append(bc)\n",
    "\n",
    "        b_c = random.choice(BC)\n",
    "        a_vec = [0.0 for _ in range(self.n_helpers)]\n",
    "        sel_helper_idxs = np.random.permutation(self.n_helpers)[0:n]\n",
    "        for helper_idx in list(sel_helper_idxs):\n",
    "            a_vec[helper_idx] = 1.0\n",
    "        action = [k] + a_vec + [b_c]\n",
    "        return action\n",
    "\n",
    "    def constrain(self, D, v_n, v_t, z_n, z_t):\n",
    "      duration = D/(v_n-v_t) - (z_n - z_t)/(v_n-v_t)\n",
    "      return duration\n",
    "\n",
    "    def action2index(self, action):\n",
    "        \"\"\"\n",
    "        Convert action from nulti-dimension format to index format\n",
    "        \"\"\"\n",
    "        if len(action) != len(self.act_dims):\n",
    "            raise Exception(\"Shape Error\")\n",
    "        \n",
    "        act_idx = action[0] - 1\n",
    "        for i in range(1, len(self.act_dims)-1):\n",
    "            act_idx = act_idx * self.act_dims[i] + action[i]\n",
    "        act_idx = act_idx * 3 + (action[-1] - 1)\n",
    "        return act_idx\n",
    "\n",
    "    def index2action(self, act_idx):\n",
    "        \"\"\"\n",
    "        Convert action from index format to multi-dimension format\n",
    "        \"\"\"\n",
    "        action = []\n",
    "        action_bw = act_idx % 3 \n",
    "        act_idx = (act_idx - action_bw)/3\n",
    "        #action_bw = action_bw + 1\n",
    "        for i in range(len(self.act_dims)-1, 1, -1):\n",
    "            ai = act_idx % self.act_dims[i-1]\n",
    "            action.append(int(ai))\n",
    "            act_idx = (act_idx - ai) / self.act_dims[i-1]\n",
    "        \n",
    "        action.append(int(act_idx))\n",
    "        action = [action_bw+ 1] + action\n",
    "        action.reverse()\n",
    "        action[0] = action[0] + 1\n",
    "        return action\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = TaskOffloadEnv(n_helpers=5,\n",
    "                        rc=100,\n",
    "                        max_f=100,\n",
    "                        max_c=100,\n",
    "                        max_l=100,\n",
    "                        alpha1=1.0,\n",
    "                        alpha2=1.0,\n",
    "                        alpha3 = 1.0,\n",
    "                        B = 4*10**13,\n",
    "                        v_min = 0,\n",
    "                        v_max = 50,\n",
    "                        seed=1)\n",
    "    action = env.sam_action()\n",
    "    idx = env.action2index(action)\n",
    "    recovered = env.index2action(idx)\n",
    "    print(idx)\n",
    "    print(action, recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CElNZbxxp4Y5"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "55gQZae-p6xe"
   },
   "outputs": [],
   "source": [
    "n_helpers = 5\n",
    "rc = 100.0\n",
    "max_f = 6e8\n",
    "max_c = 5e-3\n",
    "max_l = 3e6\n",
    "alpha1 = 1.0\n",
    "alpha2 = 1.0\n",
    "alpha3 = 1.0\n",
    "B = 4*10**13\n",
    "seed = 1\n",
    "v_min = 0\n",
    "v_max = 10\n",
    "num_episodes = 50\n",
    "max_step_per_episode = 500\n",
    "batch_size = 32\n",
    "max_eps = 1.0\n",
    "epsilon = max_eps\n",
    "min_eps = 0.1\n",
    "gpu_idx = -1\n",
    "log_dir = \"../log\"\n",
    "\n",
    "CHECKPOINT = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdNACAUCp_8U"
   },
   "source": [
    "#Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215564,
     "status": "ok",
     "timestamp": 1658121263856,
     "user": {
      "displayName": "Linh Nguyen Thi Hoai",
      "userId": "09006753140999843619"
     },
     "user_tz": -420
    },
    "id": "_zzoImX7qBKf",
    "outputId": "2889dfc3-194e-4c9a-ad47-96b53a670927"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MY PC\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "C:\\Users\\MYPC~1\\AppData\\Local\\Temp/ipykernel_6544/2096914043.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np_array = np.array(minibatch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP 0 - Step 19  || Total 0.14047 - Comp -0.15028 - Cost 0.00010358784133602585 - Anten 0.00010821428571428572 - Connect 0.29090909090909095 - Flag 0 - eps 1.0\n",
      "EP 0 - Step 39  || Total 0.27217 - Comp -0.15025 - Cost 0.0002653886888848585 - Anten 8.178571428571429e-05 - Connect 0.4227272727272728 - Flag 0 - eps 0.9992002799440072\n",
      "EP 0 - Step 59  || Total 0.07502 - Comp -0.15037 - Cost 0.006380700400579459 - Anten 0.00010107142857142854 - Connect 0.2318181818181818 - Flag 0 - eps 0.9972037767260468\n",
      "EP 0 - Step 79  || Total -0.10037 - Comp -0.15013 - Cost 0.00018468029537658666 - Anten 8.892857142857144e-05 - Connect 0.05 - Flag 0 - eps 0.9952112627234414\n",
      "EP 0 - Step 99  || Total 0.34861 - Comp -0.15011 - Cost 0.005768542149268319 - Anten 9.321428571428573e-05 - Connect 0.5045454545454545 - Flag 0 - eps 0.9932227299653352\n",
      "EP 0 - Step 119  || Total 0.17706 - Comp -0.14992 - Cost 0.0002517004491746106 - Anten 7.678571428571431e-05 - Connect 0.3272727272727274 - Flag 0 - eps 0.9912381704967991\n",
      "EP 0 - Step 139  || Total 0.13532 - Comp -0.14993 - Cost 0.0010572317895028301 - Anten 9.607142857142859e-05 - Connect 0.2863636363636364 - Flag 0 - eps 0.9892575763787981\n",
      "EP 0 - Step 159  || Total 0.15450 - Comp -0.14989 - Cost 9.766001886870102e-05 - Anten 9.82142857142857e-05 - Connect 0.3045454545454546 - Flag 0 - eps 0.9872809396881612\n",
      "EP 0 - Step 179  || Total 0.14904 - Comp -0.14998 - Cost 0.0009342768437202105 - Anten 9.107142857142857e-05 - Connect 0.3 - Flag 0 - eps 0.9853082525175486\n",
      "EP 0 - Step 199  || Total 0.17692 - Comp -0.15010 - Cost 0.009285828141641429 - Anten 0.00011249999999999998 - Connect 0.33636363636363636 - Flag 0 - eps 0.9833395069754194\n",
      "EP 0 - Step 219  || Total -0.02127 - Comp -0.15020 - Cost 0.0028355510673461304 - Anten 9.178571428571429e-05 - Connect 0.13181818181818183 - Flag 0 - eps 0.9813746951860012\n",
      "EP 0 - Step 239  || Total 0.16741 - Comp -0.15017 - Cost 0.0005508670864894262 - Anten 9.392857142857143e-05 - Connect 0.3181818181818181 - Flag 0 - eps 0.9794138092892584\n",
      "EP 0 - Step 259  || Total 0.07168 - Comp -0.14995 - Cost 0.0010504076890466758 - Anten 8.464285714285716e-05 - Connect 0.22272727272727275 - Flag 0 - eps 0.9774568414408605\n",
      "EP 0 - Step 279  || Total 0.25023 - Comp -0.14989 - Cost 0.004370206233129674 - Anten 9.107142857142857e-05 - Connect 0.40454545454545454 - Flag 0 - eps 0.9755037838121503\n",
      "EP 0 - Step 299  || Total 0.28620 - Comp -0.14998 - Cost 0.000132489831846795 - Anten 8.107142857142859e-05 - Connect 0.43636363636363645 - Flag 0 - eps 0.9735546285901135\n",
      "EP 0 - Step 319  || Total 0.20396 - Comp -0.14987 - Cost 0.0006745864880638305 - Anten 8.178571428571429e-05 - Connect 0.35454545454545455 - Flag 0 - eps 0.9716093679773469\n",
      "EP 0 - Step 339  || Total 0.41643 - Comp -0.15002 - Cost 0.0016698353519863875 - Anten 0.0001082142857142857 - Connect 0.5681818181818182 - Flag 0 - eps 0.9696679941920274\n",
      "EP 0 - Step 359  || Total 0.20457 - Comp -0.14988 - Cost 5.215583846050965e-05 - Anten 8.892857142857143e-05 - Connect 0.35454545454545455 - Flag 0 - eps 0.9677304994678808\n",
      "EP 0 - Step 379  || Total 0.08513 - Comp -0.15014 - Cost 0.001041069457796596 - Anten 9.107142857142859e-05 - Connect 0.23636363636363636 - Flag 0 - eps 0.9657968760541503\n",
      "EP 0 - Step 399  || Total 0.07135 - Comp -0.15016 - Cost 0.0011679544067872236 - Anten 9.392857142857143e-05 - Connect 0.22272727272727275 - Flag 0 - eps 0.9638671162155658\n",
      "EP 0 - Step 419  || Total 0.21471 - Comp -0.15020 - Cost 0.00777376580315659 - Anten 8.607142857142859e-05 - Connect 0.3727272727272727 - Flag 1 - eps 0.961941212232314\n",
      "EP 0 - Step 439  || Total 0.27175 - Comp -0.15011 - Cost 0.0008123601417310541 - Anten 0.00010107142857142856 - Connect 0.4227272727272727 - Flag 0 - eps 0.9600191564000061\n",
      "EP 0 - Step 459  || Total 0.04418 - Comp -0.14988 - Cost 0.0013348036771242081 - Anten 9.535714285714288e-05 - Connect 0.1954545454545455 - Flag 1 - eps 0.9581009410296466\n",
      "EP 0 - Step 479  || Total 0.07016 - Comp -0.10368 - Cost 0.003395101028404079 - Anten 7.464285714285715e-05 - Connect 0.17727272727272728 - Flag 0 - eps 0.9561865584476045\n",
      "EP 0 - Step 499  || Total 0.42356 - Comp -0.13862 - Cost 0.0013915475063144805 - Anten 0.0001082142857142857 - Connect 0.5636363636363636 - Flag 0 - eps 0.954276000995581\n",
      "Episode 0 end\n",
      "EP 1 - Step 19  || Total 0.04462 - Comp -0.15011 - Cost 0.0006854321717037859 - Anten 8.464285714285713e-05 - Connect 0.1954545454545455 - Flag 0 - eps 0.9523692610305795\n",
      "EP 1 - Step 39  || Total 0.27962 - Comp -0.15025 - Cost 0.0018909589871716171 - Anten 8.821428571428573e-05 - Connect 0.4318181818181818 - Flag 0 - eps 0.9504663309248748\n",
      "EP 1 - Step 59  || Total -0.01939 - Comp -0.15026 - Cost 0.0008968297301299868 - Anten 9.107142857142857e-05 - Connect 0.13181818181818183 - Flag 1 - eps 0.9485672030659826\n",
      "EP 1 - Step 79  || Total 0.26176 - Comp -0.15032 - Cost 0.0015131650850848625 - Anten 8.392857142857143e-05 - Connect 0.4136363636363637 - Flag 0 - eps 0.9466718698566293\n",
      "EP 1 - Step 99  || Total 0.18146 - Comp -0.15021 - Cost 9.44624826884822e-05 - Anten 9.607142857142859e-05 - Connect 0.33181818181818185 - Flag 0 - eps 0.9447803237147213\n",
      "EP 1 - Step 119  || Total 0.01151 - Comp -0.15005 - Cost 0.002029190514969231 - Anten 9.678571428571426e-05 - Connect 0.16363636363636364 - Flag 0 - eps 0.9428925570733151\n",
      "EP 1 - Step 139  || Total 0.08016 - Comp -0.15021 - Cost 0.0014016807676630277 - Anten 8.178571428571429e-05 - Connect 0.23181818181818184 - Flag 0 - eps 0.9410085623805863\n",
      "EP 1 - Step 159  || Total -0.05038 - Comp -0.15008 - Cost 0.00025951539862077304 - Anten 8.25e-05 - Connect 0.1 - Flag 0 - eps 0.9391283320998\n",
      "EP 1 - Step 179  || Total 0.08810 - Comp -0.15016 - Cost 0.002603533813340445 - Anten 8.821428571428573e-05 - Connect 0.2409090909090909 - Flag 0 - eps 0.9372518587092803\n",
      "EP 1 - Step 199  || Total 0.35224 - Comp -0.15015 - Cost 0.0021081297132270363 - Anten 8.107142857142859e-05 - Connect 0.5045454545454545 - Flag 0 - eps 0.93537913470238\n",
      "EP 1 - Step 219  || Total 0.27762 - Comp -0.15002 - Cost 0.004123035985096523 - Anten 8.892857142857143e-05 - Connect 0.4318181818181818 - Flag 0 - eps 0.9335101525874523\n",
      "EP 1 - Step 239  || Total 0.12238 - Comp -0.15002 - Cost 0.000285464277804125 - Anten 7.67857142857143e-05 - Connect 0.2727272727272728 - Flag 0 - eps 0.9316449048878183\n",
      "EP 1 - Step 259  || Total 0.20406 - Comp -0.15019 - Cost 0.00024231279009337113 - Anten 9.107142857142857e-05 - Connect 0.35454545454545455 - Flag 0 - eps 0.929783384141738\n",
      "EP 1 - Step 279  || Total 0.12603 - Comp -0.15029 - Cost 0.000886001623836211 - Anten 0.00010535714285714284 - Connect 0.2772727272727273 - Flag 0 - eps 0.927925582902382\n",
      "EP 1 - Step 299  || Total 0.00907 - Comp -0.15038 - Cost 0.00867863632257106 - Anten 8.607142857142859e-05 - Connect 0.16818181818181818 - Flag 0 - eps 0.9260714937377992\n",
      "EP 1 - Step 319  || Total 0.10388 - Comp -0.15051 - Cost 0.00010133551327390316 - Anten 9.607142857142857e-05 - Connect 0.25454545454545446 - Flag 0 - eps 0.9242211092308891\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "epsilon_decay = 0.9999\n",
    "env = TaskOffloadEnv(n_helpers,rc,max_f,max_c,max_l,alpha1,alpha2,alpha3,B, v_min, v_max, seed)\n",
    "nS = env.env_dims\n",
    "#print(nS)\n",
    "nA = env.num_actions\n",
    "#print(nA)\n",
    "learning_rate = 1e-3\n",
    "discount_rate = gamma\n",
    "dqn = DoubleDeepQNetwork(nS, nA, learning_rate, discount_rate, epsilon, min_eps, epsilon_decay)\n",
    "log_total_reward_ddqn, log_comp_reward_ddqn, log_cost_reward_ddqn, log_anten_reward_ddqn, log_connect_reward_ddqn = [], [], [], [], []\n",
    "log_Flag = []\n",
    "epsilon = max_eps\n",
    "Flag = []\n",
    "frame_count = 0\n",
    "for episode in range(num_episodes):\n",
    "\tstate = env.reset()\n",
    "\tstate = np.reshape(state, [1, nS])\n",
    "\tfor step in range(max_step_per_episode):\n",
    "\t\tframe_count +=1\n",
    "\t\taction, Flag = dqn.action(env, state)\n",
    "\t\tnstate, reward, done = env.step(action)\n",
    "\t\tnstate = np.reshape(nstate, [1, nS])\n",
    "\n",
    "\t\tdqn.store(state, action, reward[0], nstate, done) # Resize to store in memory to pass to .predict\n",
    "\t\tstate = nstate\n",
    "\n",
    "\t\t#Experience Replay\n",
    "\t\tif len(dqn.memory) > batch_size:\n",
    "\t\t\tepsilon = dqn.experience_replay(env, batch_size)\n",
    "    #Update the weights after each episode (You can configure this for x steps as well\n",
    "\t\t#epsilon = max(epsilon,min_eps)\n",
    "\t\n",
    "\t\t''' \n",
    "\t\t\tAverage & Print\n",
    "\t\t'''\n",
    "\t\tif ((step) % CHECKPOINT >= CHECKPOINT - 11) and ((step) % CHECKPOINT <= CHECKPOINT - 1):\n",
    "\t\t\tif (step) % CHECKPOINT == CHECKPOINT - 11:\n",
    "\t\t\t\tlist_total_ddqn, list_comp_ddqn, list_cost_ddqn, list_anten_ddqn, list_connect_ddqn = [], [], [], [], []\n",
    "\n",
    "\t\t\t\ttotal_reward_ddqn, comp_reward_ddqn, cost_reward_ddqn, anten_reward_ddqn, connect_reward_ddqn = 0, 0, 0, 0, 0\n",
    "\n",
    "\t\t\ttotal_reward_ddqn += reward[0]\n",
    "\t\t\tcomp_reward_ddqn += reward[1] * 1.0 / alpha1\n",
    "\t\t\tcost_reward_ddqn += reward[2] * 1.0 / alpha2\n",
    "\t\t\tanten_reward_ddqn += reward[3] * 1.0 /alpha3\n",
    "\t\t\tconnect_reward_ddqn += reward[4]\n",
    "\t\t\tlist_total_ddqn.append(total_reward_ddqn)\n",
    "\t\t\tlist_comp_ddqn.append(comp_reward_ddqn)\n",
    "\t\t\tlist_cost_ddqn.append(cost_reward_ddqn)\n",
    "\t\t\tlist_anten_ddqn.append(anten_reward_ddqn)\n",
    "\t\t\tlist_connect_ddqn.append(connect_reward_ddqn)\n",
    "\n",
    "\t\t\tif (step) % CHECKPOINT == CHECKPOINT - 1:\t\t\n",
    "\t\t\t\tavg_total_ddqn = np.mean(list_total_ddqn)\n",
    "\t\t\t\tavg_comp_ddqn = np.mean(list_comp_ddqn)\n",
    "\t\t\t\tavg_anten_ddqn = np.mean(list_anten_ddqn)\n",
    "\t\t\t\tavg_cost_ddqn = np.mean(list_cost_ddqn)\n",
    "\t\t\t\tavg_connect_ddqn = np.mean(list_connect_ddqn)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tlog_total_reward_ddqn.append(avg_total_ddqn)\n",
    "\t\t\t\tlog_comp_reward_ddqn.append(avg_comp_ddqn)\n",
    "\t\t\t\tlog_cost_reward_ddqn.append(avg_cost_ddqn)\n",
    "\t\t\t\tlog_anten_reward_ddqn.append(avg_anten_ddqn)\n",
    "\t\t\t\tlog_connect_reward_ddqn.append(avg_connect_ddqn)\n",
    "\t\t\t\tlog_Flag.append(Flag)\n",
    "\t\t\n",
    "\t\t\tif (step) % CHECKPOINT == CHECKPOINT - 1:\n",
    "\t\t\t\tprint(\"EP {} - Step {}  || Total {:.5f} - Comp {:.5f} - Cost {} - Anten {} - Connect {} - Flag {} - eps {}\".\\\n",
    "\t\t\t\t\tformat(episode, step, avg_total_ddqn, avg_comp_ddqn, avg_cost_ddqn, anten_reward_ddqn, avg_connect_ddqn, Flag,  epsilon))\n",
    "\t\tif step == max_step_per_episode-1:\n",
    "\t\t\tprint(f\"Episode {episode} end\")\n",
    "\t\t\tdone = True\n",
    "\t\tif reward[0] >= 4000:  # Condition to consider the task solved\n",
    "\t\t\tprint(\"Break at step {}!\".format(step))\n",
    "\t\t\tdone = True\n",
    "\t\t\tbreak\n",
    "\tdqn.update_target_from_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNGYxg8XqMVM"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fyQkFnWqMX0"
   },
   "source": [
    "#Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKWe1VUDqNcf"
   },
   "outputs": [],
   "source": [
    "#f = \"/content/drive/Shareddrives/Duong-LinhHUST/New_version/Separation\"\n",
    "num = 25*num_episodes\n",
    "name_path = DDQN_test.csv\n",
    "with open('name_path', 'w') as f: \n",
    "    write = csv.writer(f) \n",
    "    for i in range(num):\n",
    "         write.writerow([log_total_reward_ddqn[i], log_comp_reward_ddqn[i], log_cost_reward_ddqn[i], log_anten_reward_ddqn[i], log_connect_reward_ddqn[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBt9qz6_qPJ9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p1C40qeqPM5"
   },
   "source": [
    "#Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1658121263859,
     "user": {
      "displayName": "Linh Nguyen Thi Hoai",
      "userId": "09006753140999843619"
     },
     "user_tz": -420
    },
    "id": "0jchfzu_qQit",
    "outputId": "eef5306f-45df-4d28-9f1c-85dc01f34c0b"
   },
   "outputs": [],
   "source": [
    "idxs = list(range(len(log_total_reward_ddqn)))\n",
    "_idxs = np.linspace(0, len(idxs)-1, 7)\n",
    "x_idxs = [idxs[int(i)] for i in _idxs]\n",
    "x_names = np.array(x_idxs)\n",
    "plt.xticks(ticks=x_idxs, labels=x_names)\t\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(log_total_reward_ddqn, \"^-\", label=' DDQL: rewards', linewidth=1.75)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\" Total Reward\")\n",
    "plt.legend(loc = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6cye8pSqh6L"
   },
   "source": [
    "# Plot episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1658121263860,
     "user": {
      "displayName": "Linh Nguyen Thi Hoai",
      "userId": "09006753140999843619"
     },
     "user_tz": -420
    },
    "id": "90fF_rbfqpdr",
    "outputId": "2579a505-9bbb-4250-c8ed-9bdd933ea270"
   },
   "outputs": [],
   "source": [
    "aver_ddqn = []\n",
    "step = (len(log_total_reward_ddqn)+1)/num_episodes\n",
    "for i in range(0, len(log_total_reward_ddqn), int(step)):\n",
    "  ddqn_eps_i = log_total_reward_ddqn[i:i+int(step)-1]\n",
    "  aver_ddqn.append(np.mean(ddqn_eps_i))\n",
    "  i += 1\n",
    "plt.grid()\n",
    "plt.plot(aver_ddqn, \"^-\", label=' DDQN: rewards', linewidth=1.75)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\" Total Reward\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DDQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
